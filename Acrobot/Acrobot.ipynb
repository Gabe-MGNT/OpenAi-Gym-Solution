{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot (Simple DeepQ Learning + Replay Memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import imageio \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pendulm is described on [gym](https://www.gymlibrary.dev/environments/classic_control/acrobot/) as : \"The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\"\n",
    "\n",
    "In this task things are made easy since we only have 3 actions (easier than pendulum which had an \"infinity\"). These 3 actions are : apply -1 torque, apply 0 torque and apply 1 torque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation state is a vector of 6 numbers : cosine of theta1, sine of theta1, cosine of theta2, sine of theta2, angular velocity of theta1 and angular velocity of theta2.\n",
    "\n",
    "What's important is that the state is not discrete, so there is an infinite combination of state, making a QTable for such a problem is possible but not the right solution because of the infinite amount of state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a QTable is not the right solution, the only remaining is using DeepQ Learning.\n",
    "\n",
    "Instead of creating a table that will store all the Qvalues, we use a neural network to approximatethose QValues. The neural network allow us to have an infinite number of states, it will only depends on the network's weights and not a static table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepQ Learning (With replay memory)\n",
    "\n",
    "The difference between a simple DeepQ Learning alogorithm and another one using replay memory is that the first one will only learn from its last experience, unlike the other one that can learn from past experience. \n",
    "\n",
    "This simple make a huge difference, because without memory the model can't generalize well, and is very likely to not train on every possible situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory (deque)\n",
    "First create the replay memory, that will store : the current state, the action taken, the reward, the next state, and if the state is terminal.\n",
    "\n",
    "Basically, the memory doesn't have an infinite capcity (performance wise), so we setup the memory as a deque, it will automatically handle incoming experience, and throwing the older ones.\n",
    "\n",
    "\n",
    "The methods created are :\n",
    "- 'push' : it append a new experience to the memory\n",
    "- 'sample' : it will randomly take some experience (a batch) in the memory and output them, those will be used to update the network weights.\n",
    "- '__len__' : it overrides the 'len' methods to return the memory length, used to start updating network's weights when there are enough experience stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network\n",
    "\n",
    "The network can be set how you want, it doesn't need to be a very deep network, one hidden layer of 32 and 64 as input seems to be enough.\n",
    "\n",
    "\n",
    "The input_shape or input, is what is received, the observation so a vector 6 values (Shape : (6,)).\n",
    "\n",
    "The output are the QValues for the possible actions, in the case of the acrobot the action space had a size of 3 (1, 0 or -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Sequential, optimizers\n",
    "def create_model(input_shape, output_shape, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(64, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(output_shape, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepQ Learning algorithm\n",
    "\n",
    "The algorithm is pretty much the same as the QLearning one, the difference will be in the part of the QValue estimation.\n",
    "Before, we would take the QTable, update the corresponding state with the action taken, here the updates are only made the networks learns from memory.\n",
    "\n",
    "By taking a batch of past experiences we can compute the corresponding QValue, but instead of storing it in a table, we update the network's weights but training the model to associate the qvalues to a state in order to take the best action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_list = []\n",
    "\n",
    "def DeepQLearning(env, learning_rate, discount, epsilon, max_steps, episodes, batch_size_p=32):\n",
    "    \n",
    "    input_shape = env.observation_space.shape[0]\n",
    "    output_shape = env.action_space.n\n",
    "\n",
    "    model = create_model(\n",
    "        input_shape=input_shape, \n",
    "        output_shape=output_shape, \n",
    "        learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "\n",
    "    memory = ReplayMemory(capacity=100000)\n",
    "    batch_size = batch_size_p\n",
    "    \n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        state = state.reshape(1, input_shape)\n",
    "        done = False\n",
    "        reward_tot = 0\n",
    "\n",
    "        if i > 50:\n",
    "            #model.save_weights(f\"model.weights-eps{i}.h5\")\n",
    "            model.save_weights(\"model.weights.h5\", overwrite=True)\n",
    "\n",
    "        \n",
    "        for j in range (max_steps): \n",
    "\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(model.predict(state, verbose=0)[0])\n",
    "\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "            new_state = new_state.reshape(1, input_shape) \n",
    "            reward_tot += reward\n",
    "\n",
    "            memory.push(state, action, reward, new_state, done)\n",
    "            state = new_state\n",
    "\n",
    "            \n",
    "            # Only update the model if there are enough experiences in memory\n",
    "            if len(memory) >= batch_size:\n",
    "                \n",
    "                states, actions, rewards, new_states, dones = zip(*memory.sample(batch_size))\n",
    "\n",
    "                \n",
    "                dones = np.array(dones, dtype=np.bool_)\n",
    "                #states = np.array(states)\n",
    "                states = np.squeeze(states)\n",
    "                ####\n",
    "                actions = np.array(actions)\n",
    "                ####\n",
    "                \n",
    "                #new_states = np.array(new_states)\n",
    "                new_states = np.squeeze(new_states)\n",
    "                \n",
    "                targets = model.predict_on_batch(states)\n",
    "\n",
    "                #q_values_next = target_model.predict_on_batch(new_states)\n",
    "                q_values_next = model.predict_on_batch(new_states)\n",
    "\n",
    "\n",
    "                max_q_values_next = np.amax(q_values_next, axis=1)\n",
    "                \n",
    "                targets[range(batch_size), actions] = rewards + discount * max_q_values_next *  (1 - dones) \n",
    "\n",
    "                model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "                model.save_weights('model.weights.h5', overwrite=True)\n",
    "                \n",
    "\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Episodes n°:\", i, \"Epsilon:\", epsilon, \"Total reward:\", reward_tot)\n",
    "        rewards_list.append(reward_tot)\n",
    "        if epsilon > 0.001:\n",
    "            epsilon *= 0.98\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes n°: 0 Epsilon: 1.0 Total reward: -500.0\n",
      "Episodes n°: 1 Epsilon: 0.98 Total reward: -500.0\n",
      "Episodes n°: 2 Epsilon: 0.9603999999999999 Total reward: -500.0\n",
      "Episodes n°: 3 Epsilon: 0.9411919999999999 Total reward: -500.0\n",
      "Episodes n°: 4 Epsilon: 0.9223681599999999 Total reward: -500.0\n",
      "Episodes n°: 5 Epsilon: 0.9039207967999998 Total reward: -500.0\n",
      "Episodes n°: 6 Epsilon: 0.8858423808639998 Total reward: -500.0\n",
      "Episodes n°: 7 Epsilon: 0.8681255332467198 Total reward: -500.0\n",
      "Episodes n°: 8 Epsilon: 0.8507630225817854 Total reward: -500.0\n",
      "Episodes n°: 9 Epsilon: 0.8337477621301497 Total reward: -500.0\n",
      "Episodes n°: 10 Epsilon: 0.8170728068875467 Total reward: -500.0\n",
      "Episodes n°: 11 Epsilon: 0.8007313507497957 Total reward: -500.0\n",
      "Episodes n°: 12 Epsilon: 0.7847167237347998 Total reward: -500.0\n",
      "Episodes n°: 13 Epsilon: 0.7690223892601038 Total reward: -500.0\n",
      "Episodes n°: 14 Epsilon: 0.7536419414749017 Total reward: -500.0\n",
      "Episodes n°: 15 Epsilon: 0.7385691026454037 Total reward: -500.0\n",
      "Episodes n°: 16 Epsilon: 0.7237977205924956 Total reward: -500.0\n",
      "Episodes n°: 17 Epsilon: 0.7093217661806457 Total reward: -500.0\n",
      "Episodes n°: 18 Epsilon: 0.6951353308570327 Total reward: -500.0\n",
      "Episodes n°: 19 Epsilon: 0.6812326242398921 Total reward: -500.0\n",
      "Episodes n°: 20 Epsilon: 0.6676079717550942 Total reward: -500.0\n",
      "Episodes n°: 21 Epsilon: 0.6542558123199923 Total reward: -444.0\n",
      "Episodes n°: 22 Epsilon: 0.6411706960735924 Total reward: -500.0\n",
      "Episodes n°: 23 Epsilon: 0.6283472821521205 Total reward: -500.0\n",
      "Episodes n°: 24 Epsilon: 0.6157803365090782 Total reward: -500.0\n",
      "Episodes n°: 25 Epsilon: 0.6034647297788965 Total reward: -500.0\n",
      "Episodes n°: 26 Epsilon: 0.5913954351833186 Total reward: -500.0\n",
      "Episodes n°: 27 Epsilon: 0.5795675264796523 Total reward: -500.0\n",
      "Episodes n°: 28 Epsilon: 0.5679761759500592 Total reward: -259.0\n",
      "Episodes n°: 29 Epsilon: 0.5566166524310581 Total reward: -150.0\n",
      "Episodes n°: 30 Epsilon: 0.5454843193824369 Total reward: -168.0\n",
      "Episodes n°: 31 Epsilon: 0.5345746329947881 Total reward: -201.0\n",
      "Episodes n°: 32 Epsilon: 0.5238831403348924 Total reward: -211.0\n",
      "Episodes n°: 33 Epsilon: 0.5134054775281945 Total reward: -152.0\n",
      "Episodes n°: 34 Epsilon: 0.5031373679776306 Total reward: -213.0\n",
      "Episodes n°: 35 Epsilon: 0.493074620618078 Total reward: -235.0\n",
      "Episodes n°: 36 Epsilon: 0.48321312820571644 Total reward: -108.0\n",
      "Episodes n°: 37 Epsilon: 0.4735488656416021 Total reward: -243.0\n",
      "Episodes n°: 38 Epsilon: 0.46407788832877006 Total reward: -115.0\n",
      "Episodes n°: 39 Epsilon: 0.45479633056219465 Total reward: -221.0\n",
      "Episodes n°: 40 Epsilon: 0.44570040395095073 Total reward: -134.0\n",
      "Episodes n°: 41 Epsilon: 0.4367863958719317 Total reward: -113.0\n",
      "Episodes n°: 42 Epsilon: 0.42805066795449304 Total reward: -187.0\n",
      "Episodes n°: 43 Epsilon: 0.41948965459540316 Total reward: -139.0\n",
      "Episodes n°: 44 Epsilon: 0.4110998615034951 Total reward: -104.0\n",
      "Episodes n°: 45 Epsilon: 0.4028778642734252 Total reward: -149.0\n",
      "Episodes n°: 46 Epsilon: 0.39482030698795667 Total reward: -135.0\n",
      "Episodes n°: 47 Epsilon: 0.38692390084819756 Total reward: -127.0\n",
      "Episodes n°: 48 Epsilon: 0.3791854228312336 Total reward: -99.0\n",
      "Episodes n°: 49 Epsilon: 0.37160171437460887 Total reward: -118.0\n",
      "Episodes n°: 50 Epsilon: 0.3641696800871167 Total reward: -125.0\n",
      "Episodes n°: 51 Epsilon: 0.35688628648537435 Total reward: -137.0\n",
      "Episodes n°: 52 Epsilon: 0.34974856075566685 Total reward: -120.0\n",
      "Episodes n°: 53 Epsilon: 0.3427535895405535 Total reward: -109.0\n",
      "Episodes n°: 54 Epsilon: 0.33589851774974244 Total reward: -231.0\n",
      "Episodes n°: 55 Epsilon: 0.3291805473947476 Total reward: -132.0\n",
      "Episodes n°: 56 Epsilon: 0.32259693644685267 Total reward: -113.0\n",
      "Episodes n°: 57 Epsilon: 0.3161449977179156 Total reward: -115.0\n",
      "Episodes n°: 58 Epsilon: 0.3098220977635573 Total reward: -95.0\n",
      "Episodes n°: 59 Epsilon: 0.30362565580828615 Total reward: -137.0\n",
      "Episodes n°: 60 Epsilon: 0.2975531426921204 Total reward: -110.0\n",
      "Episodes n°: 61 Epsilon: 0.291602079838278 Total reward: -129.0\n",
      "Episodes n°: 62 Epsilon: 0.2857700382415124 Total reward: -103.0\n",
      "Episodes n°: 63 Epsilon: 0.2800546374766822 Total reward: -94.0\n",
      "Episodes n°: 64 Epsilon: 0.27445354472714856 Total reward: -101.0\n",
      "Episodes n°: 65 Epsilon: 0.2689644738326056 Total reward: -130.0\n",
      "Episodes n°: 66 Epsilon: 0.26358518435595346 Total reward: -107.0\n",
      "Episodes n°: 67 Epsilon: 0.25831348066883436 Total reward: -118.0\n",
      "Episodes n°: 68 Epsilon: 0.2531472110554577 Total reward: -119.0\n",
      "Episodes n°: 69 Epsilon: 0.24808426683434853 Total reward: -104.0\n",
      "Episodes n°: 70 Epsilon: 0.24312258149766156 Total reward: -91.0\n",
      "Episodes n°: 71 Epsilon: 0.2382601298677083 Total reward: -114.0\n",
      "Episodes n°: 72 Epsilon: 0.23349492727035415 Total reward: -117.0\n",
      "Episodes n°: 73 Epsilon: 0.22882502872494706 Total reward: -118.0\n",
      "Episodes n°: 74 Epsilon: 0.22424852815044813 Total reward: -98.0\n",
      "Episodes n°: 75 Epsilon: 0.21976355758743915 Total reward: -108.0\n",
      "Episodes n°: 76 Epsilon: 0.21536828643569036 Total reward: -85.0\n",
      "Episodes n°: 77 Epsilon: 0.21106092070697655 Total reward: -90.0\n",
      "Episodes n°: 78 Epsilon: 0.20683970229283702 Total reward: -142.0\n",
      "Episodes n°: 79 Epsilon: 0.20270290824698028 Total reward: -141.0\n",
      "Episodes n°: 80 Epsilon: 0.19864885008204067 Total reward: -228.0\n",
      "Episodes n°: 81 Epsilon: 0.19467587308039985 Total reward: -127.0\n",
      "Episodes n°: 82 Epsilon: 0.19078235561879187 Total reward: -92.0\n",
      "Episodes n°: 83 Epsilon: 0.18696670850641603 Total reward: -99.0\n",
      "Episodes n°: 84 Epsilon: 0.18322737433628772 Total reward: -145.0\n",
      "Episodes n°: 85 Epsilon: 0.17956282684956196 Total reward: -129.0\n",
      "Episodes n°: 86 Epsilon: 0.1759715703125707 Total reward: -95.0\n",
      "Episodes n°: 87 Epsilon: 0.1724521389063193 Total reward: -87.0\n",
      "Episodes n°: 88 Epsilon: 0.16900309612819292 Total reward: -107.0\n",
      "Episodes n°: 89 Epsilon: 0.16562303420562907 Total reward: -272.0\n",
      "Episodes n°: 90 Epsilon: 0.16231057352151648 Total reward: -114.0\n",
      "Episodes n°: 91 Epsilon: 0.15906436205108615 Total reward: -88.0\n",
      "Episodes n°: 92 Epsilon: 0.15588307481006441 Total reward: -80.0\n",
      "Episodes n°: 93 Epsilon: 0.15276541331386312 Total reward: -91.0\n",
      "Episodes n°: 94 Epsilon: 0.14971010504758586 Total reward: -123.0\n",
      "Episodes n°: 95 Epsilon: 0.14671590294663414 Total reward: -97.0\n",
      "Episodes n°: 96 Epsilon: 0.14378158488770146 Total reward: -111.0\n",
      "Episodes n°: 97 Epsilon: 0.14090595318994742 Total reward: -97.0\n",
      "Episodes n°: 98 Epsilon: 0.13808783412614847 Total reward: -91.0\n",
      "Episodes n°: 99 Epsilon: 0.13532607744362551 Total reward: -155.0\n",
      "Episodes n°: 100 Epsilon: 0.132619555894753 Total reward: -120.0\n",
      "Episodes n°: 101 Epsilon: 0.12996716477685794 Total reward: -93.0\n",
      "Episodes n°: 102 Epsilon: 0.12736782148132078 Total reward: -101.0\n",
      "Episodes n°: 103 Epsilon: 0.12482046505169436 Total reward: -72.0\n",
      "Episodes n°: 104 Epsilon: 0.12232405575066048 Total reward: -83.0\n",
      "Episodes n°: 105 Epsilon: 0.11987757463564727 Total reward: -500.0\n",
      "Episodes n°: 106 Epsilon: 0.11748002314293432 Total reward: -87.0\n",
      "Episodes n°: 107 Epsilon: 0.11513042268007563 Total reward: -117.0\n",
      "Episodes n°: 108 Epsilon: 0.11282781422647412 Total reward: -126.0\n",
      "Episodes n°: 109 Epsilon: 0.11057125794194463 Total reward: -88.0\n",
      "Episodes n°: 110 Epsilon: 0.10835983278310574 Total reward: -89.0\n",
      "Episodes n°: 111 Epsilon: 0.10619263612744362 Total reward: -119.0\n",
      "Episodes n°: 112 Epsilon: 0.10406878340489474 Total reward: -117.0\n",
      "Episodes n°: 113 Epsilon: 0.10198740773679685 Total reward: -98.0\n",
      "Episodes n°: 114 Epsilon: 0.09994765958206091 Total reward: -85.0\n",
      "Episodes n°: 115 Epsilon: 0.0979487063904197 Total reward: -86.0\n",
      "Episodes n°: 116 Epsilon: 0.0959897322626113 Total reward: -99.0\n",
      "Episodes n°: 117 Epsilon: 0.09406993761735907 Total reward: -103.0\n",
      "Episodes n°: 118 Epsilon: 0.0921885388650119 Total reward: -82.0\n",
      "Episodes n°: 119 Epsilon: 0.09034476808771166 Total reward: -87.0\n",
      "Episodes n°: 120 Epsilon: 0.08853787272595742 Total reward: -86.0\n",
      "Episodes n°: 121 Epsilon: 0.08676711527143827 Total reward: -86.0\n",
      "Episodes n°: 122 Epsilon: 0.0850317729660095 Total reward: -128.0\n",
      "Episodes n°: 123 Epsilon: 0.08333113750668932 Total reward: -86.0\n",
      "Episodes n°: 124 Epsilon: 0.08166451475655553 Total reward: -72.0\n",
      "Episodes n°: 125 Epsilon: 0.08003122446142442 Total reward: -102.0\n",
      "Episodes n°: 126 Epsilon: 0.07843059997219594 Total reward: -96.0\n",
      "Episodes n°: 127 Epsilon: 0.07686198797275202 Total reward: -78.0\n",
      "Episodes n°: 128 Epsilon: 0.07532474821329699 Total reward: -76.0\n",
      "Episodes n°: 129 Epsilon: 0.07381825324903105 Total reward: -85.0\n",
      "Episodes n°: 130 Epsilon: 0.07234188818405042 Total reward: -81.0\n",
      "Episodes n°: 131 Epsilon: 0.07089505042036941 Total reward: -77.0\n",
      "Episodes n°: 132 Epsilon: 0.06947714941196202 Total reward: -110.0\n",
      "Episodes n°: 133 Epsilon: 0.06808760642372277 Total reward: -102.0\n",
      "Episodes n°: 134 Epsilon: 0.06672585429524831 Total reward: -86.0\n",
      "Episodes n°: 135 Epsilon: 0.06539133720934334 Total reward: -74.0\n",
      "Episodes n°: 136 Epsilon: 0.06408351046515648 Total reward: -92.0\n",
      "Episodes n°: 137 Epsilon: 0.06280184025585335 Total reward: -89.0\n",
      "Episodes n°: 138 Epsilon: 0.061545803450736285 Total reward: -108.0\n",
      "Episodes n°: 139 Epsilon: 0.060314887381721555 Total reward: -113.0\n",
      "Episodes n°: 140 Epsilon: 0.05910858963408712 Total reward: -92.0\n",
      "Episodes n°: 141 Epsilon: 0.05792641784140538 Total reward: -97.0\n",
      "Episodes n°: 142 Epsilon: 0.05676788948457727 Total reward: -97.0\n",
      "Episodes n°: 143 Epsilon: 0.055632531694885724 Total reward: -88.0\n",
      "Episodes n°: 144 Epsilon: 0.054519881060988006 Total reward: -86.0\n",
      "Episodes n°: 145 Epsilon: 0.05342948343976824 Total reward: -94.0\n",
      "Episodes n°: 146 Epsilon: 0.052360893770972874 Total reward: -85.0\n",
      "Episodes n°: 147 Epsilon: 0.05131367589555342 Total reward: -114.0\n",
      "Episodes n°: 148 Epsilon: 0.05028740237764235 Total reward: -95.0\n",
      "Episodes n°: 149 Epsilon: 0.0492816543300895 Total reward: -73.0\n",
      "Episodes n°: 150 Epsilon: 0.04829602124348771 Total reward: -87.0\n",
      "Episodes n°: 151 Epsilon: 0.04733010081861796 Total reward: -85.0\n",
      "Episodes n°: 152 Epsilon: 0.0463834988022456 Total reward: -63.0\n",
      "Episodes n°: 153 Epsilon: 0.04545582882620069 Total reward: -90.0\n",
      "Episodes n°: 154 Epsilon: 0.04454671224967667 Total reward: -80.0\n",
      "Episodes n°: 155 Epsilon: 0.043655778004683135 Total reward: -93.0\n",
      "Episodes n°: 156 Epsilon: 0.04278266244458947 Total reward: -97.0\n",
      "Episodes n°: 157 Epsilon: 0.04192700919569768 Total reward: -86.0\n",
      "Episodes n°: 158 Epsilon: 0.04108846901178373 Total reward: -129.0\n",
      "Episodes n°: 159 Epsilon: 0.04026669963154805 Total reward: -81.0\n",
      "Episodes n°: 160 Epsilon: 0.03946136563891709 Total reward: -80.0\n",
      "Episodes n°: 161 Epsilon: 0.03867213832613875 Total reward: -85.0\n",
      "Episodes n°: 162 Epsilon: 0.037898695559615975 Total reward: -161.0\n",
      "Episodes n°: 163 Epsilon: 0.03714072164842366 Total reward: -105.0\n",
      "Episodes n°: 164 Epsilon: 0.03639790721545518 Total reward: -93.0\n",
      "Episodes n°: 165 Epsilon: 0.035669949071146075 Total reward: -109.0\n",
      "Episodes n°: 166 Epsilon: 0.034956550089723155 Total reward: -99.0\n",
      "Episodes n°: 167 Epsilon: 0.03425741908792869 Total reward: -83.0\n",
      "Episodes n°: 168 Epsilon: 0.03357227070617012 Total reward: -79.0\n",
      "Episodes n°: 169 Epsilon: 0.032900825292046715 Total reward: -75.0\n",
      "Episodes n°: 170 Epsilon: 0.03224280878620578 Total reward: -110.0\n",
      "Episodes n°: 171 Epsilon: 0.031597952610481664 Total reward: -100.0\n",
      "Episodes n°: 172 Epsilon: 0.03096599355827203 Total reward: -74.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DeepQLearning(env, \n",
    "          learning_rate=0.01, \n",
    "          discount=0.99, \n",
    "          epsilon=1.0, \n",
    "          max_steps=500, \n",
    "          episodes=400,\n",
    "          batch_size_p=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(rewards_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "env = gym.make('Acrobot-v1', render_mode='rgb_array')\n",
    "\n",
    "state,info = env.reset()\n",
    "\n",
    "\n",
    "input_shape = env.observation_space.shape[0]\n",
    "output_shape = env.action_space.n\n",
    "\n",
    "state = state.reshape(1, input_shape)\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "        input_shape=input_shape, \n",
    "        output_shape=output_shape, \n",
    "        learning_rate=0.001\n",
    ")\n",
    "model.load_weights('model.weights.h5')\n",
    "model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate = 0.001))\n",
    "\n",
    "score = 0\n",
    "done = False\n",
    "\n",
    "stp = 0\n",
    "while stp < 300:\n",
    "\n",
    "        stp += 1\n",
    "        print(stp)\n",
    "\n",
    "        action = np.argmax(model.predict(state, verbose=0))\n",
    "\n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "        state = new_state.reshape(1, input_shape) \n",
    "\n",
    "        frame = env.render()  # Save the frame\n",
    "        for m in range(5):\n",
    "                images.append(frame)\n",
    "\n",
    "\n",
    "        if done == True:\n",
    "                break\n",
    "        score +=1\n",
    "\n",
    "env.close()\n",
    "imageio.mimsave('img/AcrobotDQN9.gif', images, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
